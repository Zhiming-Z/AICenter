# AI 中台 RAG 设计方案 v1.1

> **核心思考**：针对复杂多变的源数据，如何构建企业级 RAG，实现良好的问答效果？

---

## 目录

1. [核心问题](#一核心问题)
2. [研究方向](#二研究方向)
3. [技术选型](#三技术选型)
4. [研究成果总结](#四研究成果总结)

---

## 一、核心问题

### 1.1 问题背景

企业实际场景中，源数据具有以下特点：

- **格式多样**：PDF、PPT、Excel、CSV、Word、图片、网页等
- **内容复杂**：包含表格、图表、多级标题、跨页内容、版面复杂
- **质量不一**：扫描件、加密文档、非标准格式
- **数量庞大**：成百上千份文档，需要高效处理

**核心挑战**：如何在这样一个复杂多变的场景下，构建一个既能精准检索、又能高质量生成的 RAG 系统？

### 1.2 核心挑战

| 问题域       | 核心问题                     | 研究方向             |
| --------- | ------------------------ | ---------------- |
| **数据解析**  | 不同格式文档应采用什么解析策略？         | 解析器选型、参数优化、效果对比  |
| **效果评估**  | 如何量化评价问答效果的好坏？           | 指标体系设计、评测数据集构建   |
| **系统架构**  | 是否需要针对不同文档构建不同 pipeline？ | 统一 vs 分离架构、元数据管理 |
| **数据集构建** | 如何构建自动化评测 pipeline？      | 大模型辅助标注、人工审核机制   |

### 1.3 待研究问题矩阵

| 维度  | 问题              | 优先级 |
| --- | --------------- | --- |
| 解析  | 不同格式文档的最佳解析策略？  | 高   |
| 解析  | 版面识别方案对效果的影响？   | 中   |
| 评估  | 如何评估生成阶段答案质量？   | 高   |
| 架构  | 统一知识库 vs 分类知识库？ | 中   |
| 架构  | 元数据在检索中的作用机制？   | 中   |
| 产品  | 研究先行 vs 产品先行？   | 已解决 |

---

## 二、研究方向

### 2.1 方向一：文档解析策略研究

**研究问题**：

- 不同格式文档应采用什么解析器和参数组合？
- 版面识别方案对解析效果的影响有多大？
- 是否需要针对不同类型文档定制解析策略？

**研究方法**：

1. 选择代表性文档（PDF 表格、PPT、Excel、长文档等）
2. 使用不同解析器和参数组合进行测试
3. 通过检索指标评估解析效果

**关键参数**：

- `chunk_token_num`：128/256/512/1024
- `layout_recognize`：DeepDOC/MinerU/Docling
- `similarity_threshold`：0.1/0.2/0.3/0.5
- `top_k`：3/5/10/20

> 详细功能参考：`RAGFlow功能手册.md`

---

### 2.2 方向二：评测指标体系研究

**研究问题**：

- 现有检索指标是否足够评估问答效果？
- 是否需要针对不同问题类型设计不同指标？
- 如何评估生成阶段的答案质量？

**问题类型设计**：

| 类型   | 示例                 | 测试重点   |
| ---- | ------------------ | ------ |
| 事实查询 | "2024年营业收入是多少？"    | 精确数值检索 |
| 对比查询 | "2024年比2023年增长多少？" | 跨文档关联  |
| 汇总查询 | "各季度营业收入趋势？"       | 多数据聚合  |
| 表格查询 | "研发费用占营收比例？"       | 表格解析   |
| 概念解释 | "什么是净资产收益率？"       | 定义检索   |
| 流程查询 | "如何申请报销？"          | 步骤检索   |

**待探索指标**：

- Faithfulness（忠实度/幻觉检测）
- Answer Relevance（答案相关性）
- Context Relevance（上下文相关性）
- Semantic Similarity（语义相似度）

> 详细评测系统：`RAGFlow功能手册.md`

---

### 2.3 方向三：知识库架构研究

**研究问题**：

- 统一知识库 vs 分类知识库，哪个效果更好？
- 元数据在检索中的作用机制是什么？
- 是否需要支持对话级别的文档选择？

**现有能力**：

- 对话级别知识库选择：可关联多个知识库
- 元数据支持：支持自定义元数据字段
- 搜索级别过滤：支持按文档 ID 过滤

**待探索方向**：

- 参考 NotebookLM 的文档选择机制
- 元数据与检索效果的关联分析
- 动态文档范围调整

> 详细元数据管理：`RAGFlow功能手册.md`

---

### 2.4 方向四：产品化路径研究

**研究问题**：

- 研究先行 vs 产品先行，如何平衡？
- 如何基于真实数据持续优化？
- 如何建立可复现的优化流程？

---

## 三、技术选型

### 3.1 为何选择 RAGFlow

| 特性     | RAGFlow | 说明                                          |
| ------ | ------- | ------------------------------------------- |
| 微服务架构  | ✓       | 支持灵活的组件组合                                   |
| 多向量数据库 | ✓       | Elasticsearch/Infinity/OpenSearch/OceanBase |
| 内置评测系统 | ✓       | 支持检索指标自动计算                                  |
| 多解析器支持 | ✓       | General/Book/Table，各有适用场景                   |
| 开源活跃   | ✓       | 持续更新，社区支持                                   |

### 3.2 实施策略对比

| 策略       | 周期   | 优点        | 缺点     |
| -------- | ---- | --------- | ------ |
| **产品先行** | 2-4周 | 快速交付、数据真实 | 可能走弯路  |
| **研究先行** | 4-6周 | 方案成熟、风险低  | 交付较慢   |
| **混合并行** | 持续   | 兼顾两者      | 需要较多人力 |

### 3.3 推荐方案：混合并行策略

```
┌─────────────────────────────────────────────────────────┐
│ 技术研究组                     产品开发组                 │
├─────────────────────────────────────────────────────────┤
│ - RAGFlow 深度研究              - 快速部署 MVP           │
│ - 解析方法测试                  - 上线基础功能            │
│ - 评估体系建立                  - 收集真实数据            │
│ - 参数优化实验                  - 迭代优化                │
└─────────────────────────────────────────────────────────┘
                          │
                          ▼
              定期对齐会议 (每周/双周)
                          │
                          ▼
              ┌───────────────────────┐
              │ 技术指导产品优化      │
              │ 数据验证研究方向      │
              └───────────────────────┘
```

**实施要点**：

1. 快速上线 MVP（2-4 周）
2. 并行开展技术研究（持续）
3. 基于真实数据迭代优化
4. 逐步完善功能模块

> 详细实施步骤：`RAG实施计划.md`

---

## 四、研究成果总结

### 4.1 已完成：部署与架构理解

**研究成果**：通过 Docker 和源码部署，已完整掌握 RAGFlow 的技术架构

**核心发现**：

- RAGFlow 采用微服务架构，支持灵活的组件组合
- 支持多种向量数据库（Elasticsearch/Infinity/OpenSearch/OceanBase）
- 内置完整的评测系统，支持检索指标自动计算
- 文档解析支持多种解析器，各有适用场景

### 4.2 已完成：解析器机制研究

**研究成果**：理解了 RAGFlow 的文档解析流程和三种核心解析器

**核心发现**：

| 解析器         | 适用场景               | 特点                               |
| ----------- | ------------------ | -------------------------------- |
| **General** | 通用文档（PDF/DOCX/TXT） | 支持多种版面识别（DeepDOC/MinerU/Docling） |
| **Book**    | 长文档、书籍、手册          | 自动处理目录、层级结构                      |
| **Table**   | 表格数据（Excel/CSV）    | 逐行分块、数据类型推断                      |

### 4.3 已完成：评测体系探索

**研究成果**：掌握了 RAGFlow 内置评测系统的使用方法和指标体系

**核心发现**：

**检索指标**（需标注 `relevant_chunk_ids`）：

- Precision：检索精确度
- Recall：检索召回率
- F1 Score：综合评价
- Hit Rate：命中率
- MRR：首位倒数排名

**生成指标**：

- 答案长度、是否有答案
- 执行时间、Token 用量

### 4.4 待完成研究

| 方向      | 状态  | 下一步                 |
| ------- | --- | ------------------- |
| 解析策略优化  | 待启动 | 构建测试集，对比不同配置        |
| 生成指标评估  | 待启动 | 探索 Faithfulness 等指标 |
| 元数据效果验证 | 待启动 | 分析元数据对检索的影响         |
| 高级优化技术  | 待启动 | Reranker、混合检索等      |

---

## 参考资料

### 相关文档

- **功能手册**：`RAGFlow功能手册.md` - RAGFlow 各功能详解、源码路径、操作参考
- **实施计划**：`RAG实施计划.md` - 时间线、步骤、验证标准、资源需求

### 核心源码路径

```
后端核心:
├── api/ragflow_server.py              # Flask 应用入口
├── api/apps/
│   ├── kb_app.py                      # 知识库 API
│   ├── dialog_app.py                  # 对话 API
│   ├── document_app.py                # 文档 API
│   ├── evaluation_app.py              # 评估 API
│   └── sdk/dataset.py, doc.py         # SDK API
├── api/db/
│   ├── db_models.py                   # 数据库模型
│   └── services/
│       ├── evaluation_service.py      # 评估服务
│       └── document_service.py        # 文档服务（含元数据）

文档解析:
├── rag/svr/task_executor.py           # 解析器工厂
├── rag/app/
│   ├── naive.py                       # general 解析器
│   ├── book.py                        # book 解析器
│   └── table.py                       # table 解析器

评估:
├── rag/benchmark.py                   # 基准测试工具
└── rag/nlp/search.py                  # 搜索过滤
```

---

*文档版本: v1.1*
*创建日期: 2026-01-16*
*基于 RAGFlow 开源项目*
